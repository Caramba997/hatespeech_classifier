{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_arrays():\n",
    "    df_1 = pd.read_csv(\"datasets/1_no_stopwords.csv\")\n",
    "    df_3 = pd.read_csv(\"datasets/3_no_stopwords.csv\")\n",
    "    df_5 = pd.read_csv(\"datasets/5_no_stopwords.csv\")\n",
    "    df_6 = pd.read_csv(\"datasets/6_no_stopwords.csv\")\n",
    "    \n",
    "    df = df_1.append(df_3)\n",
    "    df = df.append(df_5)\n",
    "    df = df.append(df_6)\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True, stratify=df[\"is_hatespeech\"])\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = load_data_arrays()\n",
    "\n",
    "\n",
    "# Convert the test- and train-DataFrames to Tensorflow Datasets\n",
    "\n",
    "train_labels = np.eye(2)[train_df['is_hatespeech'].values]\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(np.asarray(train_df['text'].values, dtype=str), tf.string),\n",
    "            tf.cast(train_labels, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "test_labels = np.eye(2)[test_df['is_hatespeech'].values]\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(np.asarray(test_df['text'].values, dtype=str), tf.string),\n",
    "            tf.cast(test_labels, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(64,), dtype=string, numpy=\n",
      "array([b'oh well go get malware yourself itll stop going wikipedia get real admins on eh good idea chump',\n",
      "       b'ways improve rosie fellner hi im kudpung turskellies thanks creating rosie fellner ive tagged page using page curation tools issues fix please take moment return article address tagged issues the tags removed another editor issues mention addressed if questions leave comment talk page or editing help talk volunteers teahouse',\n",
      "       b'pov some seems bit pov especially paragraph begins in march 1990 the person wrote obviously trying evenhanded delicate matters like this would probably better cite conflicting view responsible sources talk 0131 nov 18 2004 utc',\n",
      "       b'i see disregarded request and based email copied editors talk page resubmitted content article i going revert edit ip editor got first please refrain posting inappropriate material wikipedia familiarize relevant policies links provided above wikipedia content must based majority views verifiable published sources emails individuals',\n",
      "       b'bring hello tommy remember me im back asshole',\n",
      "       b'what fuck its okay white proud',\n",
      "       b'and following exchange i answered charge i admitted participating discussion included advice get palestinians sanctioned although i believe tegwarrior necessarily mean insinuate i participated discussion i remember anything discussion probably read email the part galls moreschi deleted denial without explanation',\n",
      "       b'hey kerry loser gore 1408 14 feb 2005 utc',\n",
      "       b'article tell people game does do do this',\n",
      "       b'no problem my question is add wvrw state listing page giving benefit doubt fcc might little slow leave fcc updates',\n",
      "       b'wikipediaarticles deletiondavid hahn nebraska\\nwikipedia place personal website it encyclopedia designed document notable people you may post whatever information wish user page',\n",
      "       b'look hoes man fuck hoes niggah lets get smoked',\n",
      "       b'rt easy bake ovens teaching girls role life since 1963 im sexist hilarious',\n",
      "       b'ok show parts edits without references i gave references everything btw i know from wikipedias rules cant even say article hitler murderer write article form shows nop neonazi antisemitic wikipedia place criticism criticism whole article and again stop changing article personal beliefes',\n",
      "       b'everybody it excuse oppressive behavior',\n",
      "       b'blocking mardyks excellent work shii we cant kind getting us think maya actually say prophecies we insulted him offended abused ethical persistant block mother fukkers taking entire santa fe public library system great preemptive strike also there may others kind sympathize indians these people actually love earth without reliable sources we kicked asses right write their history interpret their sacred teachings however please we need college students indoctrinated church academia that piece stitler one exaggerated opinionated yeah use title page and means give john major jenkins section not single scholar mayanists agrees appropriated theory kind hypocrisy arrogance wiki about we get away it continuing use power censor free thinkers like mardyks kind sony pictures paying us tickets let us know many want free popcorn too whoopee best wishes jimini cricket 9712326228',\n",
      "       b'purdy el salvador\\nthere contradiction wording wpmosflag officially represented emphasized section titled biographical use however section titled use flags sportspeople says or declared nation his sporting nationality question june 5 it really according looks like pretty clear intent mos reverting wasting time',\n",
      "       b'blockk mee blockk mee im gonna make personal attacks get business wasnt even fucking sooooooooo shut fuck take like man okay godpussy want know you deserve nothing i hope you get less',\n",
      "       b'im sexist canadian anthem changed its tradition know men women equal',\n",
      "       b'bitch you whiny ass attention whore bitch go choke cock',\n",
      "       b'yo fuck amigo settlin score faggot bean picker',\n",
      "       b'prove giving prominence term arabian gulf it saying controversially used by way sockpuppet',\n",
      "       b'i strongly agree mrzman issue proposals verify ban credentials run aground respective opponents leaving checkuser verification idea one really shot though significant opposition must overcome first',\n",
      "       b'bro u fucking retard u actually think u better viewers twitch broget',\n",
      "       b'talkback im sure whether following discussion i thought id let know ive responded linked thread \\xe2\\x80\\x94 talk',\n",
      "       b'something needs immediate attention randi works well',\n",
      "       b'rt this bitch tried give dick hickey',\n",
      "       b'1 feb 2004 utc\\nthats issue the issue deleting large amounts factual content without bothering dispute even discuss detail replacing large amount pov commentary speculation this supposed encyclopedia entry oped page 1023',\n",
      "       b'removal forum link folkestone page just wondering removed folkestone forums link folkestone page onfolkestone forums link seems tad unfair whilst one gets stay doesnt especially folkestone forums portal built include photographs information town members town councillor date information towns development',\n",
      "       b'i saw man actual conversation bird haha',\n",
      "       b'go mapsgooglecom type pussy go pussy france 2412923734',\n",
      "       b'as dec 3 2013 article says 20 kb ram both main article summary size later correctly states 5 kb ram 35 kb accessible basic programming the 20 kb references replaced 5kb',\n",
      "       b'rt no officer i drunk driving i swerving hoes',\n",
      "       b'again i began saying i think word perfect needed worked rather simply excised npov comment edit and actually i similar remark raul hilbergs mouth later article i believe entirely addresses povnpov my point say anschluss cannot viewed military annexation austria view simply would misunderstand essence would case provide anything like absolution effect changing subject hence wording tends conceal in case make claim without remarking limited analytic value obvious propaganda value would set aside number basic facts presented elsewhere article simply reiterate argument without quotation attribution case without pointing immediately contradicts pov subtle sort certainly always resort attributing remarks some others gallup level presenting opinions little inform reader grasp essential truths for reasons i think think weasel proposal achieves neutral although blandly equivocally stated one drops ones guard therefore less likely call out 0752 21 jun 2005 utc',\n",
      "       b'tell dem thot bitches im not right',\n",
      "       b'wikiproject podcasting started\\nthe project way if like join feel free add name members list project page thanks i hope joing us soon fon',\n",
      "       b'for profit its profit right if verified included as profit',\n",
      "       b'what personal attacks there people general mayhem editting pages joke you think thats contributing page no isnt genmay never shut people keeps reinserting back there',\n",
      "       b'rt follow spree cunts just retweet followmebeau',\n",
      "       b'i ask take information account reviewing block',\n",
      "       b'well perhaps could explain exactly think article supposed be frankly im baffled and totally obscure huge vijay prasad quote serious wpweight problem if quote supposed indicate article is please let know ill move articles vehicles pov thanks',\n",
      "       b'this sculpture living artist whose work noted copyrighted this would seem preclude ever taking photograph would satisfy wikipedias guidelines though someone corrects point im happy upload one',\n",
      "       b'no seriously he is banned now saying feeding troll idiot',\n",
      "       b'are saying nauru shithole are criticizingu2026',\n",
      "       b'go contribute something anything expand stub rewrite grammar section random article even cursory examination edit history evidence activities get removal content dislike attacking sourcing without ever actually adding improving sourcing you get endless talk page circlejerk arguments end repetitious dispute resolution procedures you net drain wikipedian resources im going play that youre right one thing philosopher numbered citations needed lead sentence fact wellestablished throughout article i leave appease the preceding comment deleted userdavid gerard',\n",
      "       b'barnstar thanks much nice see back snowman',\n",
      "       b'oh girl started arguments me she stuck nose belong i believe argument yvesnimmo but like i said situation settled i apologized thanks',\n",
      "       b'if get comment talk page without contacting outside talk page substantive issue i issue complete abject apology \\xc2\\xabtalk\\xc2\\xbb',\n",
      "       b'i think pertinent april 15 also anniversary belfast blitz many things clear connection event stuff including patriots day one could argue maybe theres stronger connection made patriots day thats speculation possibly original research regards',\n",
      "       b'fuck you your fuckin stupid im black fuck you\\nfuck uou\\nsdjasfldjgs\\ndgsdglknfdts',\n",
      "       b'your repretition hoax give gobbels result you showing true biased colors discussion',\n",
      "       b'dear mongo i said before i think concerns perspectives valid merit may i respectfully ask please label users work pov pushing coatrack labeling tends destroy users enjoyment editing encyclopedia significantly reduces peoples motivation find compromise very valid positions lets create friendly hospitable supportive work environment encourage erik david a yourself dheyward every user may motivated contribute building first draft the final consensus version may possibly even probably gonna look different first or second third rough drafts may i advise patience calm lets please try less emotional cool gentle supportive other especially may otherwise disagree please take personal attack im saying emotional rather logical person i believe emotional logical rational like are i believe emotions important powerful role play debates i tend get emotional sometimes please lets try different approach give chance succeed thanks',\n",
      "       b'section needed roots differences\\nan important point brought article since us nation imigrants americans ethnically racially different counterparts rest world in words differences created temperment personality differences those left verse those left behind this could help explain stereotypical american traits aggression material indulgence userbrando03',\n",
      "       b'agree i lived singapore 3 years studying im indian btw caucasian students i knew found term best mildly amusing found annoying demeaning couple really upset called strangers friends but common understanding origins saw inherently racist term even speakers using context',\n",
      "       b'in history world every building ever hit jumbo jet flight collapsed does hurt moron',\n",
      "       b'one night vegas',\n",
      "       b'idiot barek you obviously spending much time vandalizing pages instead watching news reading news one you instead vandalizing pages go read it huh spent little time search guys name find plenty news reports pity you seems enjoy vandalizing pages instead helping',\n",
      "       b'is possible civil exchange you i attacking rather stating facts according us law i know live village south london may familiar posted is indeed serious complaint initiated furthermore alleging david e henderson link posted possible violation copyright absurd untrue i wrote article online newsletter completely unrelated wikipedia personally i think action reveals inability grasp facts context',\n",
      "       b'are female i ther new rold',\n",
      "       b'oh hello fnarf apologise sincerest condolences disruptiveness i sarcastic',\n",
      "       b'help me begging you',\n",
      "       b'you quite right usually underlying image copyrighted if make perfect sketch photograph francis bacon painting covered copyright original painting but coins nobody owns copyright actual coin what british library claims copyright picture coin if coin twodimensional would ignore add copyright tag but threedimensional so lawyer said oil painting then cant that but sketch photograph sketch going original picture it certainly seems odd me i lawyer i forgot say i got copy rumbles book the reign cnut i chance read yet i cant tell going useful talk',\n",
      "       b'look see pcid like peer review redefined it now denyse oleary like peer review either pcid meet wprs trying censor cheating',\n",
      "       b'ephblog personal blog officially recognized college its format unique ephblog name generic catchy enough sound like could official college blog id guess name reason blog readership does regardless vast majority content ephblog reflects views david kane whether frequently implores others participate more the subject matter may span all things eph viewpoints certainly not i think elevates ephblog status personal blog make ephblog eligible inclusion page'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(64, 2), dtype=int32, numpy=\n",
      "array([[1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0]])>)\n"
     ]
    }
   ],
   "source": [
    "# print(train_dataset)\n",
    "print(next(iter(train_dataset)))\n",
    "# print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kw = pd.read_csv(\"datasets/bad_words.csv\")\n",
    "kw_vocab = set()\n",
    "for idx, item in df_kw.iterrows():\n",
    "  kw_vocab.add(item[0])\n",
    "kw_vocab_len = len(kw_vocab) + 1\n",
    "kw_layer = tf.keras.layers.TextVectorization(vocabulary=list(kw_vocab))\n",
    "kw_layer.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "ec_layer = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "ec_layer.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(kw_layer([\"You are a nice little idiot\"]))\n",
    "# print(ec_layer([\"You are a nice little idiot\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   4/2717 [..............................] - ETA: 7:11:46 - loss: 0.6912 - accuracy: 0.6523"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(model.summary())\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m0.00005\u001b[39m), loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projekte\\NLP\\Final Project\\hatespeech_classifier\\.venv2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Projekte\\NLP\\Final Project\\hatespeech_classifier\\.venv2\\lib\\site-packages\\keras\\engine\\training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1211\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1212\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1213\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1214\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1215\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1216\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1217\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1218\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32me:\\Projekte\\NLP\\Final Project\\hatespeech_classifier\\.venv2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Projekte\\NLP\\Final Project\\hatespeech_classifier\\.venv2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    907\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 910\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    912\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    913\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32me:\\Projekte\\NLP\\Final Project\\hatespeech_classifier\\.venv2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    939\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    940\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    941\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32me:\\Projekte\\NLP\\Final Project\\hatespeech_classifier\\.venv2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3128\u001b[0m   (graph_function,\n\u001b[0;32m   3129\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projekte\\NLP\\Final Project\\hatespeech_classifier\\.venv2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1955\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1957\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1958\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1961\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m     args,\n\u001b[0;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1964\u001b[0m     executing_eagerly)\n\u001b[0;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32me:\\Projekte\\NLP\\Final Project\\hatespeech_classifier\\.venv2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    605\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    607\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    610\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    611\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32me:\\Projekte\\NLP\\Final Project\\hatespeech_classifier\\.venv2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "input_layer = layers.Input(shape=(1,), dtype=(tf.string))\n",
    "seq_layer = kw_layer(input_layer)\n",
    "seq2_layer = ec_layer(input_layer)\n",
    "concat_layer = layers.Concatenate(axis=1)([seq_layer, seq2_layer])\n",
    "concat_layer = layers.Embedding(input_dim=len(kw_layer.get_vocabulary() + ec_layer.get_vocabulary()), output_dim=64, mask_zero=True)(concat_layer)\n",
    "concat_layer = layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(concat_layer)\n",
    "concat_layer = layers.Bidirectional(tf.keras.layers.LSTM(32))(concat_layer)\n",
    "concat_layer = layers.Dense(64, activation='relu')(concat_layer)\n",
    "concat_layer = layers.Dropout(0.5)(concat_layer)\n",
    "output_layer = layers.Dense(2, activation='softmax')(concat_layer)\n",
    "\n",
    "model = tf.keras.Model(name=\"hatespeech_detector\", inputs=input_layer, outputs=output_layer)\n",
    "# print(model.summary())\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, epochs=5, validation_data=test_dataset, validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hatespeech_detector\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, None)        0           ['input_2[0][0]',                \n",
      " ization)                                                         'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 64)     183552      ['text_vectorization[2][0]']     \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 64)     183552      ['text_vectorization[3][0]']     \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirectional  (None, None, 128)   66048       ['embedding_2[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_6 (Bidirectional  (None, None, 128)   66048       ['embedding_3[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirectional  (None, 64)          41216       ['bidirectional_4[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_7 (Bidirectional  (None, 64)          41216       ['bidirectional_6[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           4160        ['bidirectional_5[0][0]']        \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64)           4160        ['bidirectional_7[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64)           0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 2)            130         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 2)            130         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4)            0           ['dense_6[0][0]',                \n",
      "                                                                  'dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 2)            10          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 590,222\n",
      "Trainable params: 590,222\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 30s 44ms/step - loss: 0.2849 - accuracy: 0.9173\n",
      "Test Loss: 0.28492414951324463\n",
      "Test Accuracy: 0.9172821640968323\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06653643 0.9334636 ]\n",
      " [0.9860053  0.01399465]]\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text without padding.\n",
    "sample_text = ['You are such a stupid fucking whore',\n",
    "               'I would not recommend this movie.']\n",
    "predictions = model.predict(np.array(sample_text))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ein Unterverzeichnis oder eine Datei mit dem Namen \"models\" existiert bereits.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_19_layer_call_fn while saving (showing 5 of 40). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/binary_features\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/binary_features\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002A80EC23490> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002A80EC43C10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002A8153DBBB0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002A8153E5580> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002A80FD59DF0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002A810FA0C40> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002A816608DF0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002A818A42C40> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "!mkdir models\n",
    "model.save('models/binary_features')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9a9cabc42f5adcc0f3294cfdbbe9ffcb3a15664c9675865bec9473cb4197e8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('.venv2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
