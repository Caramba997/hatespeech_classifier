{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_arrays():\n",
    "    df_1 = pd.read_csv(\"datasets/1_no_stopwords.csv\")\n",
    "    df_3 = pd.read_csv(\"datasets/3_no_stopwords.csv\")\n",
    "    df_5 = pd.read_csv(\"datasets/5_no_stopwords.csv\")\n",
    "    df_6 = pd.read_csv(\"datasets/6_no_stopwords.csv\")\n",
    "    \n",
    "    df = df_1.append(df_3)\n",
    "    df = df.append(df_5)\n",
    "    df = df.append(df_6)\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True, stratify=df[\"is_hatespeech\"])\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = load_data_arrays()\n",
    "\n",
    "\n",
    "# Convert the test- and train-DataFrames to Tensorflow Datasets\n",
    "\n",
    "train_labels = np.eye(2)[train_df['is_hatespeech'].values]\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(np.asarray(train_df['text'].values, dtype=str), tf.string),\n",
    "            tf.cast(train_labels, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "test_labels = np.eye(2)[test_df['is_hatespeech'].values]\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(np.asarray(test_df['text'].values, dtype=str), tf.string),\n",
    "            tf.cast(test_labels, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(64,), dtype=string, numpy=\n",
      "array([b'i committing vandalism are i hereby formally requesting stop harassing wikipedia if persist i everything power banned thank you',\n",
      "       b'i already said i again',\n",
      "       b'ah yes i forgot subst thanks care share',\n",
      "       b'on twitter know talking to oh woman software dev oh lordy',\n",
      "       b'yes sophia summary bullet points side dispute describes arguments excellent idea enables noninvolved editors understand nature dipute hopefully bring unexplored perspectives resolution \\xe2\\x80\\xa2',\n",
      "       b'rt we want skinny bitches get out we want big booty bitches',\n",
      "       b'every good story starts so bitch lol',\n",
      "       b'oh noes insult internets get moron being blocked affect slightest i life encouraging users like sp however might backfire whatever topics like edit here i would quite like see would deal various issues really understanding policy all blp npov or tpo fucking anything really especially hes learned trick misguided pompous arses like best even acknowledge concerns presented others play mommy mommy card unless course incompetent evidently are case content dispute two would sight behold surely like two baboons flinging shit ostensibly visiting museum',\n",
      "       b'your submission articles creation jane wenhamjones jane wenhamjones submitted articles creation created the article assessed startclass recorded articles talk page you may like take look grading scheme see improve article\\nyou welcome continue making quality contributions wikipedia note loggedin user create articles yourself post request however welcome continue submitting work articles creation if questions welcome ask help desk if would like help us improve process please consider thank helping improve wikipedia\\n\\xe2\\x80\\x94 you need establish article lead section person considered notable check wpcreative guidelines \\xe2\\x80\\x94',\n",
      "       b'i reviewed article placed hold there one reference left fixed whole thing passes its reference that according checklinks expire once fixed i pass article \\xe2\\x80\\xa2 talk \\xe2\\x80\\xa2',\n",
      "       b'black dahlia murder is not melodic death metal they are deathcore pussies black dahlia murder is not melodic death metal they farthest band genre black dahlia murder pieces shit scene bitches trying metal along deathcoregrindcore bands like job for a cowboy suicide silence etc fuck them they true metal never be need stick knowamerican eagle mtv lame trends melodic death metal scar symmetry in flames sonic syndicate disarmonia mundi melodic death metal real music disgrace music like deathcore grindcore i continue change black dahlia cuntmurders genre deathcoregrindcore i give shitim letting stupid scene bitches take away thing mainstream destroyed thats amazing genre melodic death metal so fuck trendy conformists go fucking die black dahlia murder deathcore pussies need beheaded deathcore bands ruining metal deathcore biggest piece shit ive ever heardit even considered music bunch pussy scene kids trying metal deathcore grindcore two worst forms music ever invented so yeah listen deathcore enjoy hearing music tempo beat whatsoever pretty much hit random shit drums play random notes guitar vocalist terrible random screaming pig squels dont forget hillarious millions breakdowns put songs dathis song is called big fucking tree ya boiwe wit american eagleya boi hair flippy thing rawr rawr rawr im so lyk hardcores omgz gages and snake bites rawwwrrr fuck job cowboy black dahlia murder trendy pieces shit melodic death metal owns all there really much difference deathcore band matter bunch pussies trying fit new trend trying metal band so please try tell ohyour stupidthe bands arent the same ok maybejust maybe job for a cowboys vocalist 3 pig squels second vocalist black dahlia murder i said it i guess same oh dear fuck and badmy bad white chapel actually 200 breakdowns minute rather suicide silences 100 breakdowns minute my bad dude right along deathcore grindcore bands same but seriously genre pure shit another trend music industry bands following make top fit rest bands days i mean i nothing every band certain genre sounding anything music genre produce pure shit done living human earththen i draw line it take much talent shittty raw screaming hit random notes guitar hit double bass pedal like 500bpm another reason i take bands genre seriously hillarious excuses band logo they try oh hard harddcoreeeee i also find funny pussy bands names like anal cunt dead death bloody dead hand fact is band produces music genre sucks trying fitin ya boi lets hardcore dance rahhhh yaaaaaa grindcore deathcore metal douchebags give upyou metal never be go fall pool aids fuck black dahlia murder im giving upthis insane im letting mother fuckers take away one thing i ruined',\n",
      "       b'anybody you were harrassing me',\n",
      "       b'again i began saying i think word perfect needed worked rather simply excised npov comment edit and actually i similar remark raul hilbergs mouth later article i believe entirely addresses povnpov my point say anschluss cannot viewed military annexation austria view simply would misunderstand essence would case provide anything like absolution effect changing subject hence wording tends conceal in case make claim without remarking limited analytic value obvious propaganda value would set aside number basic facts presented elsewhere article simply reiterate argument without quotation attribution case without pointing immediately contradicts pov subtle sort certainly always resort attributing remarks some others gallup level presenting opinions little inform reader grasp essential truths for reasons i think think weasel proposal achieves neutral although blandly equivocally stated one drops ones guard therefore less likely call out 0752 21 jun 2005 utc',\n",
      "       b'get twisted shit bow bitches',\n",
      "       b'speedy deletion supportspace a tag placed supportspace requesting speedily deleted wikipedia this done section g11 criteria speedy deletion article seems blatant advertising promotes company product group service person would need fundamentally rewritten order become encyclopedia article please read guidelines spam well wikipediabusiness faq information if think notice placed error may contest deletion adding top article just existing speedy deletion db tag coupled adding note articles talk page explaining position aware tagged speedy deletion article meets criterion may deleted without delay please remove speedy deletion tag yourself hesitate add information article would would render conformance wikipedias policies guidelines',\n",
      "       b'wikipediarequests page protection could take look wikipediarequests page protectioncurrent requests reduction protection level request operation midnight climax thanks uqaqtuq talk sunasuttuq',\n",
      "       b'i also noticed selfadmitted antizionist support palestinian right return sidestepping ridiculous position palestinians entitled right return jews expelled arab countries exactly find fit edit articles arbiter impartiality why insistent continuously removing entries instead discussing directly talk page are really coward positions refuse discuss issues me',\n",
      "       b'im big fan pizza baked ziti ive calling suhbahroh years i heard pronounced spahroh recent episode saturday night live its new yorkbased show talking new yorkbased restaurant chain certainly credible',\n",
      "       b'sorarias name also i might eders girlfriends name wrong',\n",
      "       b'lamar odom little bitch its pathetic',\n",
      "       b'air forces memorial bob removed link air forces memorial without charge kind act christian charity i send images inscriptions memorial people australia canada etc etc self promotion believe dealing with is kind spamming reckon obviating waht definition selfinterest',\n",
      "       b'you like attack people defaming get piece medicine cant take it be man actually know is if take lesson father',\n",
      "       b'title reference\\nit also refers tony sees svetlana\\n8018710832',\n",
      "       b'you cut spam i made one violation get high horse 6655141100',\n",
      "       b'if two ganging id get report first get banned71174142108',\n",
      "       b'true but you cant prove that we did itin',\n",
      "       b'welp scouter twat either reported deactivated must vote red get protecters illegal aliens',\n",
      "       b'oh hello fnarf apologise sincerest condolences disruptiveness i sarcastic',\n",
      "       b'my talk page dont post messages talk page unless absolutely necessary eg give required notice in addition ive taken recent administrative action you ask it thanks',\n",
      "       b'nazi asshole always be',\n",
      "       b'ok whatever it matter much figure likely change just say i really understand math one sources give places army losses 60 killeddated september 27',\n",
      "       b'thank senator graham ive critical support illegal aliens defen\\xe2\\x80\\xa6',\n",
      "       b'attention brought administrators here vesey\\xc2\\xa0review me',\n",
      "       b'wouldnt lovely like edits lede nice start', b'looked section',\n",
      "       b'the hate speech part seems slighty pov melodramatic context certainly ecyclopedic compared previous header i objection change islamism islam i feel done i hope explains questions please feel free ask rtinp23',\n",
      "       b'verified accts ok idea much administrative overhead there better ways ive talked past',\n",
      "       b'a last note although i strongly disagree you i take matter since seems 2to1 adament keep status quo i want start edit war dhdr',\n",
      "       b'oh mythat trolling equivalent wmd a meme mass destruction',\n",
      "       b'you like cunts here utter cunt get nasty ailment die you fucking prick',\n",
      "       b'just realise valid render invalid the problem arguments provided you evidenced overall reaction rather nonreaction everyme',\n",
      "       b'coaches dont put positions successful offensive struggles put gardner program trash',\n",
      "       b'insane\\nmos def notable necro\\nmos def gay rapper rhymes soft insults white people goes white women\\nall writing hiphop excludes necro everything\\nu talk underground hiphop dont mention him\\nu talk eastcoast hiphop dont mention necro\\nu r lost sauce\\nall speaking black experience ignoring white experience\\nu r racist come\\nand fan thinks necro best represented\\ni dont like articles\\nand feel false remove',\n",
      "       b'request do versions fileheterocarpus ensiferjpg better resolution danke brasiliensis',\n",
      "       b'species1be hidn pl',\n",
      "       b'ill look tonight ive noticed lot improvements made it',\n",
      "       b'ronald mcdonald gay fart fetish ronald mcdonald raped ass massive penis farted face gay 21912783147',\n",
      "       b'im spamming article idiot the website selling anything related link topics',\n",
      "       b'pterosaurs format\\nhey im working revamp list pterosaurs ive experimenting slightly different format displaying images relation table way i previously interfered tables sorting function if could check section page tell looks compared tables get back me id grateful',\n",
      "       b'poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop poop',\n",
      "       b'thanks please reblock me desire take part site allows person linked perverted websites criminals racism none deleted see petris history still you block abusing multipile accounts making personal attacks block evasion dont give fuck get done delete shit associated name please',\n",
      "       b'fuck u bitch fuck u bitch',\n",
      "       b'its long time summerphd remember me its while probably dont my original account greenbaypackersfan09 that first wikipedia account i 11 year old i first used account 2009 14 lasted used 2012 im 17 years old now i remember i would used create lot articles notably actors starred shows i watched wizards waverly place hannah montana etc we first crossed paths 2009 and parted ways 2012 looking last edit account userpage nominated good number deletion some got deleted if look talk page greenbaypackers account notice i actually got vindicated good number articles nominated deletion \\xe2\\x80\\x93 lot articles intact fact expanded pictures sources etc harry shum simon curtis article i forgot password account later used sockpuppets which i remember blocked for lol so been ive well ive started editing broad multitude articles productively id argue i editing productively account well tad overzealous i edit articles current events public figures politicians etc ive learned lot wikipedias rules made good number productive edits i stopped say amazing still around some people retire well anyway wanted let know',\n",
      "       b'redirect talkyes mika nakashima album',\n",
      "       b'7seriesbot we got thanks talk\\xe2\\x86\\x92 \\xe2\\x86\\x90track',\n",
      "       b'my major takeaways discussion scholarly sources refer name before lasted 700 years therefore excellent claim primary topic title i feel arguments move sufficiently compelling even close no consensus never mind keep current location',\n",
      "       b'once again leave alone you bothering sake bothering me you even need reply just stop typing pressing save usertalk please thank you',\n",
      "       b'rt i would rather lose 100k followers standing others bad position gain 100 keeping mouth shut fo\\xe2\\x80\\xa6',\n",
      "       b'yo all right look all i trying add back discography section sesshomaru kept removing after reverting couple times told add unverifiable information i removed 2009 album someone added then told needed references i added reference billboardcom and im blocked',\n",
      "       b'the original discussion here',\n",
      "       b'saying quote submitted civil rights laws passed protect rights white men apply them later submitted fictional additional text incorrect yet source asking one',\n",
      "       b'thought thing',\n",
      "       b'pic copyright\\nhi welcome\\ni live bristol ive wikipedia since january 2003 i want make small point requirement say pics public domain whatever i looked notredame de paris article i notice copyright pd notice i click mag glass\\ni understand details gnu free documentation licence luckily i worry many thousand pics ive put wp ive declared pd the rest pd web pages ive got permission use them\\nat rate mention pics status must given page get click mag glass ive started using automatic message msgpd underneath message have look saab 340 see pd message produces\\nalso could look image description pages userarpingstonepic list ideas messages\\nbest wishes',\n",
      "       b'bobby bitch make body flip know know karate bitch'], dtype=object)>, <tf.Tensor: shape=(64, 2), dtype=int32, numpy=\n",
      "array([[1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1]])>)\n"
     ]
    }
   ],
   "source": [
    "# print(train_dataset)\n",
    "print(next(iter(train_dataset)))\n",
    "# print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kw = pd.read_csv(\"datasets/bad_words.csv\")\n",
    "kw_vocab = set()\n",
    "for idx, item in df_kw.iterrows():\n",
    "  kw_vocab.add(item[0])\n",
    "kw_vocab_len = len(kw_vocab) + 1\n",
    "kw_layer = tf.keras.layers.TextVectorization(vocabulary=list(kw_vocab))\n",
    "kw_layer.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projekte\\NLP\\Final Project\\hatespeech_classifier\\.venv2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717/2717 [==============================] - 164s 56ms/step - loss: 0.6351 - accuracy: 0.8591 - val_loss: 0.5720 - val_accuracy: 0.8953\n",
      "Epoch 2/5\n",
      "2717/2717 [==============================] - 148s 55ms/step - loss: 0.5053 - accuracy: 0.9229 - val_loss: 0.4742 - val_accuracy: 0.9078\n",
      "Epoch 3/5\n",
      "2717/2717 [==============================] - 162s 60ms/step - loss: 0.4227 - accuracy: 0.9251 - val_loss: 0.4160 - val_accuracy: 0.9016\n",
      "Epoch 4/5\n",
      "2717/2717 [==============================] - 149s 55ms/step - loss: 0.3669 - accuracy: 0.9246 - val_loss: 0.3688 - val_accuracy: 0.9047\n",
      "Epoch 5/5\n",
      "2717/2717 [==============================] - 149s 55ms/step - loss: 0.3267 - accuracy: 0.9258 - val_loss: 0.3405 - val_accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "input_layer = layers.Input(shape=(1,), dtype=(tf.string))\n",
    "seq_layer = kw_layer(input_layer)\n",
    "seq_layer = layers.Embedding(input_dim=len(kw_layer.get_vocabulary()), output_dim=64, mask_zero=True)(seq_layer)\n",
    "seq_layer = layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(seq_layer)\n",
    "seq_layer = layers.Bidirectional(tf.keras.layers.LSTM(32))(seq_layer)\n",
    "seq_layer = layers.Dense(64, activation='relu')(seq_layer)\n",
    "seq_layer = layers.Dropout(0.5)(seq_layer)\n",
    "seq_layer = layers.Dense(2, activation='softmax')(seq_layer)\n",
    "output_layer = layers.Dense(2, activation='softmax')(seq_layer)\n",
    "\n",
    "model = tf.keras.Model(name=\"hatespeech_keywords\", inputs=input_layer, outputs=output_layer)\n",
    "# print(model.summary())\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, epochs=5, validation_data=test_dataset, validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hatespeech_keywords\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 64)          183552    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 128)        66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 295,112\n",
      "Trainable params: 295,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680/680 [==============================] - 17s 25ms/step - loss: 0.3114 - accuracy: 0.9260\n",
      "Test Loss: 0.31142401695251465\n",
      "Test Accuracy: 0.9259796142578125\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11463743 0.8853626 ]\n",
      " [0.97191095 0.02808903]]\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text without padding.\n",
    "sample_text = ['You are such a stupid fucking whore',\n",
    "               'I would not recommend this movie.']\n",
    "predictions = model.predict(np.array(sample_text))\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9a9cabc42f5adcc0f3294cfdbbe9ffcb3a15664c9675865bec9473cb4197e8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('.venv2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
